{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic1_1_NN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1ySdnqs-7Sad6eOHvwA-u-hQOMlB8dWsj",
      "authorship_tag": "ABX9TyPZYbss6tK/MZRSdu9pt+oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sk-welldan/ai-handson/blob/master/basic1_1_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_k1j14YHsmw",
        "colab_type": "text"
      },
      "source": [
        "### ニューラルネットワーク\n",
        "ニューラルネットワーク（Nural Network）とは、  \n",
        "ここでは、最もシンプルなニューラルネットワークの実装を学びます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JGL7rvj5HeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 必要なライブラリを導入\n",
        "import numpy as np\n",
        "\n",
        "# 共通クラスを導入\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/AI-handson')\n",
        "from common.activations import softmax\n",
        "from common.gradient import numerical_gradient\n",
        "from common.loss import cross_entropy_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC2x-tIVOPxF",
        "colab_type": "text"
      },
      "source": [
        "### ニューラルネットワークは何を行なっているの？\n",
        "* 「ニューラルネットワークにおいて勾配を求める」とは、損失$L$の$W$に対する偏微分$\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}$を求めるということです。  \n",
        "    例、  \n",
        "    ${\\bf W}=\\begin{pmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{pmatrix}$  \n",
        "  \n",
        "    $\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}=\\begin{pmatrix} \\frac{\\partial L}{\\partial {w_{11}}} &\n",
        "                                                                                      \\frac{\\partial L}{\\partial {w_{12}}} &\n",
        "                                                                                      \\frac{\\partial L}{\\partial {w_{13}}} \\\\\n",
        "                                                                                      \\frac{\\partial L}{\\partial {w_{21}}} &\n",
        "                                                                                      \\frac{\\partial L}{\\partial {w_{22}}} &\n",
        "                                                                                      \\frac{\\partial L}{\\partial {w_{23}}}\n",
        "                                                                                      \\end{pmatrix}$  \n",
        "                                                                                            \n",
        "  \n",
        "    \n",
        "* $\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}$は、${\\bf W}$のそれぞれの要素が微小量変化したときの$L$の変化量を表します。\n",
        "* 機械学習では、$L$を最小化する${\\bf W}$の値を求めることを目的として${\\bf W}$の値を更新していきます。\n",
        "\n",
        "\n",
        "### [実装してみよう！]\n",
        "* 最もシンプルなニューラルネットワークにおいて、予測(predict)、損失(loss)、勾配(gradient)を計算する以下のクラスを完成させてみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04VWtQnNORIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class simpleNet:\n",
        "    def __init__(self, seed=1234):\n",
        "        np.random.seed(seed)\n",
        "        self.W = np.random.randn(2,3)#Wの初期値をランダムに作成します\n",
        "        \n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        予測関数\n",
        "        ソフトマックス関数を使用します\n",
        "        \"\"\"\n",
        "        z = np.dot(x, self.W)\n",
        "        y = softmax(z)\n",
        "        return y\n",
        "    \n",
        "    def loss(self, x, t):\n",
        "        \"\"\"\n",
        "        損失関数\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        loss = cross_entropy_error(y, t)\n",
        "        return loss\n",
        "    \n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"\n",
        "        勾配計算関数\n",
        "        \"\"\"\n",
        "        grads={}\n",
        "        f = self.loss\n",
        "        W = self.W\n",
        "        grads[\"W\"] = numerical_gradient(f, x, W, t)\n",
        "\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK1gRgIZOVra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}